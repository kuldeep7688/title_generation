{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1hw3jbNrEuJn7Fl75AkQKkL-DKqU63AAk","authorship_tag":"ABX9TyO9hxugQJk6xsnLSQ/L4Ec0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nGZaaRJXNTZ0","executionInfo":{"status":"ok","timestamp":1669681476215,"user_tz":300,"elapsed":2688,"user":{"displayName":"Ye Ma","userId":"00302420574097801041"}},"outputId":"24f8de19-94b4-47c1-d48d-3cf6c52ad0d9"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["% of rare words in vocabulary:  65.63486000863529\n","Size of vocabulary in X = 10348\n","% of rare words in vocabulary: 87.36530172413794\n","Size of vocabulary in Y = 470\n"]},{"output_type":"execute_result","data":{"text/plain":["(1977, 1977)"]},"metadata":{},"execution_count":11}],"source":["import pandas as pd\n","import string\n","import re\n","import nltk\n","nltk.download('stopwords')\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","import numpy as np\n","import tensorflow\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input, Concatenate, TimeDistributed\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.models import Model\n","import tensorflow.keras.utils as ku\n","from tensorflow.keras.utils import to_categorical\n","\n","PUNCTUATION = r'[\\,\\;\\?\\.\\!\\:\\\"\\(\\)]'\n","\n","#Training data preparation\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/train_split.csv')\n","text = df['content'].tolist()\n","\n","#Preprocessing: remove stopwords and punctuation, stemming\n","def preprocess_data(text):\n","    stopwords_english = stopwords.words('english')\n","    stemmer = PorterStemmer()\n","    \n","    texts_clean = []\n","    for word in text:\n","        if (word not in stopwords_english and  \n","                word not in PUNCTUATION):\n","            stem_word = stemmer.stem(word)  # stemming word\n","            texts_clean.append(stem_word)\n","\n","    return texts_clean\n","df['content_clean'] = preprocess_data(text)\n","df['title'] = df['title'].apply(lambda x : 'sostok '+ x + ' eostok')\n","\n","#Test data preparation\n","df2 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/test_split.csv')\n","text2 = df2['content'].tolist()\n","df2['content_clean'] = preprocess_data(text2)\n","df2['title'] = df2['title'].apply(lambda x : 'sostok '+ x + ' eostok')\n","\n","\n","#Tokenization for contents\n","x_tokenizer = Tokenizer() \n","x_tokenizer.fit_on_texts(list(df['content_clean']))\n","\n","#Check rare words (occur less than 5 times)\n","thresh = 5\n","cnt = 0\n","tot_cnt = 0\n","\n","for key, value in x_tokenizer.word_counts.items():\n","    tot_cnt = tot_cnt + 1\n","    if value < thresh:\n","        cnt = cnt + 1\n","    \n","print(\"% of rare words in vocabulary: \", (cnt / tot_cnt) * 100)\n","\n","#Tokenization with frequent words only\n","x_tokenizer = Tokenizer(num_words = tot_cnt - cnt) \n","x_tokenizer.fit_on_texts(list(df['content_clean']))\n","\n","# Convert content sequences to integer sequences for training_content and test_content\n","train_seq = x_tokenizer.texts_to_sequences(df['content_clean']) \n","test_seq = x_tokenizer.texts_to_sequences(df2['content_clean'])\n","\n","# Pad zero upto maximum length\n","max_content_len = 15\n","train_content = pad_sequences(train_seq,  maxlen=max_content_len, padding='post')\n","test_content = pad_sequences(test_seq, maxlen=max_content_len, padding='post')\n","\n","# Size of content vocabulary (+1 for padding token)\n","content_voc = x_tokenizer.num_words + 1\n","print(\"Size of vocabulary in X = {}\".format(content_voc))\n","\n","#Tokenization for titles\n","y_tokenizer = Tokenizer()   \n","y_tokenizer.fit_on_texts(list(df['title']))\n","\n","#Check rare words\n","thresh = 5\n","\n","cnt = 0\n","tot_cnt = 0\n","\n","for key, value in y_tokenizer.word_counts.items():\n","    tot_cnt = tot_cnt + 1\n","    if value < thresh:\n","        cnt = cnt + 1\n","    \n","print(\"% of rare words in vocabulary:\",(cnt / tot_cnt) * 100)\n","\n","#Tokenization with reqular words only\n","y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n","y_tokenizer.fit_on_texts(list(df['title']))\n","\n","# Convert title sequences to integer sequences for training_title and test_title\n","train_title_seq = y_tokenizer.texts_to_sequences(df['title']) \n","test_title_seq = y_tokenizer.texts_to_sequences(df2['title']) \n","\n","# Pad zero upto maximum length\n","max_title_len = 15\n","train_title = pad_sequences(train_title_seq, maxlen=max_title_len, padding='post')\n","test_title = pad_sequences(test_title_seq, maxlen=max_title_len, padding='post')\n","\n","# Size of title vocabulary\n","title_voc = y_tokenizer.num_words + 1\n","print(\"Size of vocabulary in Y = {}\".format(title_voc))\n","y_tokenizer.word_counts['sostok'],len(train_title)  \n","\n"]},{"cell_type":"code","source":["\n","##Encoder - decoder layers\n","latent_dim = 300\n","embedding_dim = 200\n","\n","# Encoder\n","encoder_inputs = Input(shape=(max_content_len, ))\n","\n","# Embedding layer\n","enc_emb = Embedding(content_voc, embedding_dim,\n","                    trainable=True)(encoder_inputs)\n","\n","# Encoder LSTM 1\n","encoder_lstm1 = LSTM(latent_dim, return_sequences=True,\n","                     return_state=True, dropout=0.4,\n","                     recurrent_dropout=0.4)\n","(encoder_output1, state_h1, state_c1) = encoder_lstm1(enc_emb)\n","\n","# Encoder LSTM 2\n","encoder_lstm2 = LSTM(latent_dim, return_sequences=True,\n","                     return_state=True, dropout=0.4,\n","                     recurrent_dropout=0.4)\n","(encoder_output2, state_h2, state_c2) = encoder_lstm2(encoder_output1)\n","\n","# Encoder LSTM 3\n","encoder_lstm3 = LSTM(latent_dim, return_state=True,\n","                     return_sequences=True, dropout=0.4,\n","                     recurrent_dropout=0.4)\n","(encoder_outputs, state_h, state_c) = encoder_lstm3(encoder_output2)\n","\n","# Set up the decoder, using encoder_states as the initial state\n","decoder_inputs = Input(shape=(None, ))\n","\n","# Embedding layer\n","dec_emb_layer = Embedding(title_voc, embedding_dim, trainable=True)\n","dec_emb = dec_emb_layer(decoder_inputs)\n","\n","# Decoder LSTM\n","decoder_lstm = LSTM(latent_dim, return_sequences=True,\n","                    return_state=True, dropout=0.4,\n","                    recurrent_dropout=0.2)\n","(decoder_outputs, decoder_fwd_state, decoder_back_state) = \\\n","    decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n","\n","# Dense layer\n","decoder_dense = TimeDistributed(Dense(title_voc, activation='softmax'))\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Define the model\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","model.summary()\n","\n","\n"],"metadata":{"id":"371a98Yrw1vc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669684804830,"user_tz":300,"elapsed":1103,"user":{"displayName":"Ye Ma","userId":"00302420574097801041"}},"outputId":"9b9ff8bb-5a1e-4a59-f56c-6762062381ea"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_9\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_16 (InputLayer)          [(None, 15)]         0           []                               \n","                                                                                                  \n"," embedding_6 (Embedding)        (None, 15, 200)      2069600     ['input_16[0][0]']               \n","                                                                                                  \n"," lstm_12 (LSTM)                 [(None, 15, 300),    601200      ['embedding_6[0][0]']            \n","                                 (None, 300),                                                     \n","                                 (None, 300)]                                                     \n","                                                                                                  \n"," input_17 (InputLayer)          [(None, None)]       0           []                               \n","                                                                                                  \n"," lstm_13 (LSTM)                 [(None, 15, 300),    721200      ['lstm_12[0][0]']                \n","                                 (None, 300),                                                     \n","                                 (None, 300)]                                                     \n","                                                                                                  \n"," embedding_7 (Embedding)        (None, None, 200)    94000       ['input_17[0][0]']               \n","                                                                                                  \n"," lstm_14 (LSTM)                 [(None, 15, 300),    721200      ['lstm_13[0][0]']                \n","                                 (None, 300),                                                     \n","                                 (None, 300)]                                                     \n","                                                                                                  \n"," lstm_15 (LSTM)                 [(None, None, 300),  601200      ['embedding_7[0][0]',            \n","                                 (None, 300),                     'lstm_14[0][1]',                \n","                                 (None, 300)]                     'lstm_14[0][2]']                \n","                                                                                                  \n"," time_distributed_3 (TimeDistri  (None, None, 470)   141470      ['lstm_15[0][0]']                \n"," buted)                                                                                           \n","                                                                                                  \n","==================================================================================================\n","Total params: 4,949,870\n","Trainable params: 4,949,870\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["#Training\n","model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n","\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n","\n","history = model.fit(\n","    [train_content, train_title[:, :-1]],\n","    train_title.reshape(train_title.shape[0], train_title.shape[1], 1)[:, 1:],\n","    epochs=20,\n","    callbacks=[es],\n","    batch_size=128,\n","    validation_data=([test_content, test_title[:, :-1]],\n","                     test_title.reshape(test_title.shape[0], test_title.shape[1], 1)[:\n","                     , 1:]),\n","    )\n"],"metadata":{"id":"sN-keHTsxVMd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669685074928,"user_tz":300,"elapsed":260166,"user":{"displayName":"Ye Ma","userId":"00302420574097801041"}},"outputId":"599930b0-57c5-47f5-d494-99074eb7f606"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","16/16 [==============================] - 86s 2s/step - loss: 2.2781 - val_loss: 1.2854\n","Epoch 2/20\n","16/16 [==============================] - 25s 2s/step - loss: 1.4256 - val_loss: 1.2556\n","Epoch 3/20\n","16/16 [==============================] - 24s 2s/step - loss: 1.3795 - val_loss: 1.2651\n","Epoch 4/20\n","16/16 [==============================] - 25s 2s/step - loss: 1.3207 - val_loss: 1.1531\n","Epoch 5/20\n","16/16 [==============================] - 27s 2s/step - loss: 1.2590 - val_loss: 1.2372\n","Epoch 6/20\n","16/16 [==============================] - 25s 2s/step - loss: 1.2383 - val_loss: 1.1195\n","Epoch 7/20\n","16/16 [==============================] - 25s 2s/step - loss: 1.2088 - val_loss: 1.1343\n","Epoch 8/20\n","16/16 [==============================] - 24s 2s/step - loss: 1.2087 - val_loss: 1.1213\n","Epoch 8: early stopping\n"]}]},{"cell_type":"code","source":["##Make predictions\n","reverse_target_word_index = y_tokenizer.index_word\n","reverse_source_word_index = x_tokenizer.index_word\n","target_word_index = y_tokenizer.word_index\n","\n","# Inference Models\n","\n","# Encode the input sequence to get the feature vector\n","encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs,\n","                      state_h, state_c])\n","\n","# Decoder setup\n","# Below tensors will hold the states of the previous time step\n","decoder_state_input_h = Input(shape=(latent_dim, ))\n","decoder_state_input_c = Input(shape=(latent_dim, ))\n","decoder_hidden_state_input = Input(shape=(max_content_len, latent_dim))\n","\n","# Get the embeddings of the decoder sequence\n","dec_emb2 = dec_emb_layer(decoder_inputs)\n","\n","# To predict the next word in the sequence, set the initial states to the states from the previous time step\n","decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2,\n","        initial_state=[decoder_state_input_h, decoder_state_input_c])\n","\n","# A dense softmax layer to generate probability distribution over the target vocabulary\n","decoder_outputs2 = decoder_dense(decoder_outputs2)\n","\n","# Final decoder model\n","decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,\n","                      decoder_state_input_h, decoder_state_input_c],\n","                      [decoder_outputs2] + [state_h2, state_c2])\n","\n","#Decode function that predicts the title: start and end tokens (sostok&eostok)\n","def decode_sequence(input_seq):\n","\n","    # Encode the input as state vectors.\n","    e_out, e_h, e_c = encoder_model.predict(input_seq)\n","\n","    # Generate empty target sequence of length 1\n","    target_seq = np.zeros((1, 1))\n","\n","    # Populate the first word of target sequence with the start word.\n","    target_seq[0, 0] = target_word_index['sostok']\n","\n","    stop_condition = False\n","    decoded_sentence = ''\n","\n","    while not stop_condition:\n","        (output_tokens, h, c) = decoder_model.predict([target_seq]\n","                + [e_out, e_h, e_c])\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_token = reverse_target_word_index[sampled_token_index]\n","\n","        if sampled_token != 'eostok':\n","            decoded_sentence += ' ' + sampled_token\n","\n","        # Exit condition: either hit max length or find the stop word.\n","        if sampled_token == 'eostok' or len(decoded_sentence.split()) \\\n","            >= max_title_len - 1:\n","            stop_condition = True\n","\n","        # Update the target sequence (of length 1)\n","        target_seq = np.zeros((1, 1))\n","        target_seq[0, 0] = sampled_token_index\n","\n","        # Update internal states\n","        (e_h, e_c) = (h, c)\n","\n","    return decoded_sentence\n","\n","# To convert sequence to original titles\n","def seq2summary(input_seq):\n","    newString = ''\n","    for i in input_seq:\n","        if i != 0 and i != target_word_index['sostok'] and i \\\n","            != target_word_index['eostok']:\n","            newString = newString + reverse_target_word_index[i] + ' '\n","\n","    return newString\n"],"metadata":{"id":"sMMKFwO6w-Wv","executionInfo":{"status":"ok","timestamp":1669685280035,"user_tz":300,"elapsed":260,"user":{"displayName":"Ye Ma","userId":"00302420574097801041"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["for i in range(len(test_content)):\n","  print(\"Predicted title:\",seq2text(test_content[i]))"],"metadata":{"id":"lC3w0cTj5wlk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from rouge import Rouge\n","ROUGE = Rouge()"],"metadata":{"id":"mBOEv4FsFIkT","executionInfo":{"status":"ok","timestamp":1669686663821,"user_tz":300,"elapsed":115,"user":{"displayName":"Ye Ma","userId":"00302420574097801041"}}},"execution_count":85,"outputs":[]},{"cell_type":"code","source":["for i in range(len(test_content)):\n","  reference = seq2summary(test_title[i])\n","  candidate = decode_sequence(test_content[i].reshape(1,max_content_len))"],"metadata":{"id":"vmM2d2db2rg_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ROUGE.get_scores(candidate, reference)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_J04kX9kGETU","executionInfo":{"status":"ok","timestamp":1669687447554,"user_tz":300,"elapsed":110,"user":{"displayName":"Ye Ma","userId":"00302420574097801041"}},"outputId":"145648e2-a44d-4b63-dc51-48bce1ea3492"},"execution_count":95,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n","  'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n","  'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]"]},"metadata":{},"execution_count":95}]}]}